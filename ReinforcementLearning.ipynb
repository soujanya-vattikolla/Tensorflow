{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soujanya-vattikolla/Tensorflow/blob/main/ReinforcementLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ADWvu7NKN2r"
      },
      "source": [
        "### Reinforcement Learning\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGCR3JWQLaQb"
      },
      "source": [
        "###Terminology\n",
        "\n",
        "**Enviornemt** In reinforcement learning tasks we have a notion of the enviornment. This is what our *agent* will explore. An example of an enviornment in the case of training an AI to play say a game of mario would be the level we are training the agent on.\n",
        "\n",
        "**Agent** an agent is an entity that is exploring the enviornment. Our agent will interact and take different actions within the enviornment. In our mario example the mario character within the game would be our agent. \n",
        "\n",
        "**State** always our agent will be in what we call a *state*. The state simply tells us about the status of the agent. The most common example of a state is the location of the agent within the enviornment. Moving locations would change the agents state.\n",
        "\n",
        "**Action** any interaction between the agent and enviornment would be considered an action. For example, moving to the left or jumping would be an action. \n",
        "\n",
        "**Reward** every action that our agent takes will result in a reward of some magnitude (positive or negative). The goal of our agent will be to maximize its reward in an enviornment. \n",
        "\n",
        "The most important part of reinforcement learning is determing how to reward the agent. After all, the goal of the agent is to maximize its rewards. This means we should reward the agent appropiatly such that it reaches the desired goal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoOJy9s4ZJJt"
      },
      "source": [
        "###Q-Learning\n",
        "\n",
        "* Q-Learning is a simple yet quite powerful technique in machine learning that involves learning a matrix of action-reward values. \n",
        "* This matrix is often reffered to as a Q-Table or Q-Matrix. The matrix is in shape (number of possible states, number of possible actions) where each value at matrix[n, m] represents the agents expected reward given they are in state n and take action m. \n",
        "* The Q-learning algorithm defines the way we update the values in the matrix and decide what action to take at each state. \n",
        "* The idea is that after a succesful training/learning of this Q-Table/matrix we can determine the action an agent should take in any state by looking at that states row in the matrix and taking the maximium value column as the action.\n",
        "\n",
        "**Consider this example.**\n",
        "\n",
        "Let's say A1-A4 are the possible actions and we have 3 states represented by each row (state 1 - state 3).\n",
        "\n",
        "| A1  | A2  | A3  | A4  |\n",
        "|:--: |:--: |:--: |:--: |\n",
        "|  0  |  0  | 10  |  5  |\n",
        "|  5  | 10  |  0  |  0  |\n",
        "| 10  |  5  |  0  |  0  |\n",
        "\n",
        "If that was our Q-Table/matrix then the following would be the preffered actions in each state.\n",
        "\n",
        "> State 1: A3\n",
        "\n",
        "> State 2: A2\n",
        "\n",
        "> State 3: A1\n",
        "\n",
        "We can see that this is because the values in each of those columns are the highest for those states!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5uLpN1yemTx"
      },
      "source": [
        "###Learning the Q-Table\n",
        "\n",
        "Our agent learns by exploring the enviornment and observing the outcome/reward from each action it takes in each state. But how does it know what action to take in each state? There are two ways that our agent can decide on which action to take.\n",
        "1. Randomly picking a valid action\n",
        "2. Using the current Q-Table to find the best action.\n",
        "\n",
        "####Updating Q-Values\n",
        "The formula for updating the Q-Table after each action is as follows:\n",
        "> $ Q[state, action] = Q[state, action] + \\alpha * (reward + \\gamma * max(Q[newState, :]) - Q[state, action]) $\n",
        "\n",
        "- $\\alpha$ stands for the **Learning Rate**\n",
        "\n",
        "- $\\gamma$ stands for the **Discount Factor**\n",
        "\n",
        "####Learning Rate $\\alpha$\n",
        "The learning rate $\\alpha$ is a numeric constant that defines how much change is permitted on each QTable update. A high learning rate means that each update will introduce a large change to the current state-action value. A small learning rate means that each update has a more subtle change. \n",
        "\n",
        "####Discount Factor $\\gamma$\n",
        "Discount factor also know as gamma ($\\gamma$) is used to balance how much focus is put on the current and future reward. A high discount factor means that future rewards will be considered more heavily.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwIl0sJgmu4D"
      },
      "source": [
        "###Q-Learning Example\n",
        "\n",
        "For this example we will use the Q-Learning algorithm to train an agent to navigate a popular enviornment from the [Open AI Gym]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSETF0zqokYr"
      },
      "source": [
        "import gym   # all you have to do to import and use open ai gym!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cH3AmCzotO1"
      },
      "source": [
        "Once you import gym you can load an enviornment using the line ```gym.make(\"enviornment\")```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKN1ScBco3dp"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')  # we are going to use the FrozenLake enviornment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SvSlmVwo8cY"
      },
      "source": [
        "There are a few other commands that can be used to interact and get information about the enviornment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF3icIeapFct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d7183d-a785-4c06-8865-8e8206ac2de5"
      },
      "source": [
        "print(env.observation_space.n)   # get number of states\n",
        "print(env.action_space.n)   # get number of actions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc9cwp03pQVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321b0f72-e5a6-4900-cd9f-115dd2172440"
      },
      "source": [
        "env.reset()  # reset enviornment to default state"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sngyjPDapUt7"
      },
      "source": [
        "action = env.action_space.sample()  # get a random action "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeEfi8xypXya"
      },
      "source": [
        "new_state, reward, done, info = env.step(action)  # take action, notice it returns information about the action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1W3D81ipdaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb537e6-fbeb-4948-ad3b-9e5dfb349c75"
      },
      "source": [
        "env.render()   # render the GUI for the enviornment "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmW6HAbQp01f"
      },
      "source": [
        "###Frozen Lake Enviornment\n",
        "\n",
        "The enviornment we loaded above ```FrozenLake-v0``` is one of the simplest enviornments in Open AI Gym. The goal of the agent is to navigate a frozen lake and find the Goal without falling through the ice (render the enviornment above to see an example).\n",
        "\n",
        "There are:\n",
        "- 16 states (one for each square) \n",
        "- 4 possible actions (LEFT, RIGHT, DOWN, UP)\n",
        "- 4 different types of blocks (F: frozen, H: hole, S: start, G: goal)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlWoK75ZrK2b"
      },
      "source": [
        "###Building the Q-Table\n",
        "The first thing we need to do is build an empty Q-Table that we can use to store and update our values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r767K4s0rR2p"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "STATES = env.observation_space.n\n",
        "ACTIONS = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAzMWGatrVIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42319284-5c91-4c29-f32f-31199be533a8"
      },
      "source": [
        "Q = np.zeros((STATES, ACTIONS))  # create a matrix with all 0 values \n",
        "Q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc_h8tLSrpmc"
      },
      "source": [
        "###Constants\n",
        "As we discussed we need to define some constants that will be used to update our Q-Table and tell our agent when to stop training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FQapdnnr6P1"
      },
      "source": [
        "EPISODES = 2000 # how many times to run the enviornment from the beginning\n",
        "MAX_STEPS = 100  # max number of steps allowed for each run of enviornment\n",
        "\n",
        "LEARNING_RATE = 0.81  # learning rate\n",
        "GAMMA = 0.96"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxrAj91rsMfm"
      },
      "source": [
        "###Picking an Action\n",
        "Remember that we can pick an action using one of two methods:\n",
        "1. Randomly picking a valid action\n",
        "2. Using the current Q-Table to find the best action.\n",
        "\n",
        "Here we will define a new value $\\epsilon$ that will tell us the probabillity of selecting a random action. This value will start off very high and slowly decrease as the agent learns more about the enviornment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUAQVyX0sWDb"
      },
      "source": [
        "epsilon = 0.9  # start with a 90% chance of picking a random action\n",
        "\n",
        "# code to pick action\n",
        "if np.random.uniform(0, 1) < epsilon:  # we will check if a randomly selected value is less than epsilon.\n",
        "    action = env.action_space.sample()  # take random action\n",
        "else:\n",
        "    action = np.argmax(Q[state, :])  # use Q table to pick best action based on current values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n-i0B7Atige"
      },
      "source": [
        "###Updating Q Values\n",
        "The code below implements the formula discussed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r7R1W6Qtnh8"
      },
      "source": [
        "# Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[new_state, :]) - Q[state, action])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__afaD62uh8G"
      },
      "source": [
        "###Putting it Together\n",
        "Now that we know how to do some basic things we can combine these together to create our Q-Learning algorithm,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGiYCiNuutHz"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "STATES = env.observation_space.n\n",
        "ACTIONS = env.action_space.n\n",
        "\n",
        "Q = np.zeros((STATES, ACTIONS))\n",
        "\n",
        "EPISODES = 1500 # how many times to run the enviornment from the beginning\n",
        "MAX_STEPS = 100  # max number of steps allowed for each run of enviornment\n",
        "\n",
        "LEARNING_RATE = 0.81  # learning rate\n",
        "GAMMA = 0.96\n",
        "\n",
        "RENDER = False # if you want to see training set to true\n",
        "\n",
        "epsilon = 0.9\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFRtn5dUu5ZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9b28e9-8438-4428-dcd5-e793e78cdcef"
      },
      "source": [
        "rewards = []\n",
        "for episode in range(EPISODES):\n",
        "\n",
        "  state = env.reset()\n",
        "  for _ in range(MAX_STEPS):\n",
        "    \n",
        "    if RENDER:\n",
        "      env.render()\n",
        "\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "      action = env.action_space.sample()  \n",
        "    else:\n",
        "      action = np.argmax(Q[state, :])\n",
        "\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "    Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[next_state, :]) - Q[state, action])\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    if done: \n",
        "      rewards.append(reward)\n",
        "      epsilon -= 0.001\n",
        "      break  # reached goal\n",
        "\n",
        "print(Q)\n",
        "print(f\"Average reward: {sum(rewards)/len(rewards)}:\")\n",
        "# and now we can see our Q values!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.11002539e-01 1.82254444e-02 1.82552622e-02 1.85106146e-02]\n",
            " [8.55338155e-03 1.37874347e-03 8.96923586e-03 2.62475703e-01]\n",
            " [1.95455278e-01 8.24638912e-03 7.05146687e-03 1.28942635e-02]\n",
            " [3.01146044e-03 6.92193859e-03 2.88744735e-03 1.32958601e-02]\n",
            " [2.56929824e-01 4.76415462e-03 1.77870311e-03 9.34759078e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.40356610e-05 5.81228239e-05 1.79726311e-02 1.01774996e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.58932597e-03 3.98316417e-03 1.09719551e-02 2.85277929e-01]\n",
            " [4.58412013e-03 5.00886739e-01 6.21504812e-03 3.41139217e-03]\n",
            " [9.60586502e-02 2.60525324e-03 5.26165176e-03 3.75290657e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.07216603e-01 3.40565022e-02 5.53779603e-01 7.77830572e-02]\n",
            " [2.29778104e-01 7.32650202e-01 2.15908243e-01 1.85535897e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Average reward: 0.31466666666666665:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo-tNznd65US",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "6252d25f-e84a-4d8b-f69f-014ad200a595"
      },
      "source": [
        "# we can plot the training progress and see how the agent improved\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_average(values):\n",
        "  return sum(values)/len(values)\n",
        "\n",
        "avg_rewards = []\n",
        "for i in range(0, len(rewards), 100):\n",
        "  avg_rewards.append(get_average(rewards[i:i+100])) \n",
        "\n",
        "plt.plot(avg_rewards)\n",
        "plt.ylabel('average reward')\n",
        "plt.xlabel('episodes (100\\'s)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}